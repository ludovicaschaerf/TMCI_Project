{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lucovica Schaerf, Antònio Mendes, Jaël Kortekaas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large part of our code is used from: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the the preprocessing pipeline to our project. \n",
    "As a first step we are importing the data and filtering out all the songs that\n",
    "we don't need for our analysis. Secondly, we will implement the 'standard' \n",
    "pipeline and, once we obtain the most common words per each album, author, year\n",
    "(...) we will move to another file to do the clustering and topic analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.collocations import *\n",
    "from nltk.collocations import BigramAssocMeasures\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import gensim\n",
    "import spacy\n",
    "\n",
    "from gensim import corpora, models\n",
    "from nltk.corpus import wordnet as wn\n",
    "from operator import itemgetter\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_dir = Path(\"./figures\")\n",
    "data_dir = Path(\"./data\")\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics, manifold\n",
    "import scipy\n",
    "from scipy import cluster\n",
    "import matplotlib as mpl \n",
    "import matplotlib.cm as cm\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['index', 'song', 'year', 'artist', 'genre', 'lyrics'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "songs = []\n",
    "\n",
    "with open('./data/lyrics.csv', 'r', encoding=\"utf-8\") as infile:\n",
    "    songs = pd.read_csv(infile)\n",
    "    \n",
    "print(songs.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>song</th>\n",
       "      <th>year</th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>116793</td>\n",
       "      <td>116793</td>\n",
       "      <td>if-i-m-dreaming-my-life</td>\n",
       "      <td>2009</td>\n",
       "      <td>david-bowie</td>\n",
       "      <td>Rock</td>\n",
       "      <td>VERSE (there)\\nWas she never there/here?\\nWas ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116794</td>\n",
       "      <td>116794</td>\n",
       "      <td>seven</td>\n",
       "      <td>2009</td>\n",
       "      <td>david-bowie</td>\n",
       "      <td>Rock</td>\n",
       "      <td>I forgot what my father said\\nI forgot what he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116795</td>\n",
       "      <td>116795</td>\n",
       "      <td>i-can-t-read</td>\n",
       "      <td>2009</td>\n",
       "      <td>david-bowie</td>\n",
       "      <td>Rock</td>\n",
       "      <td>I can't read and I can't write down\\nI don't k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116796</td>\n",
       "      <td>116796</td>\n",
       "      <td>thursday-s-child</td>\n",
       "      <td>2009</td>\n",
       "      <td>david-bowie</td>\n",
       "      <td>Rock</td>\n",
       "      <td>All of my life I've tried so hard\\nDoing my be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116797</td>\n",
       "      <td>116797</td>\n",
       "      <td>survive</td>\n",
       "      <td>2009</td>\n",
       "      <td>david-bowie</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Oh, my\\nNaked eyes\\nI should have kept you\\nI ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67509</td>\n",
       "      <td>67509</td>\n",
       "      <td>child-in-time</td>\n",
       "      <td>1972</td>\n",
       "      <td>deep-purple</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Sweet child in time\\nYou'll see the line\\nThe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67510</td>\n",
       "      <td>67510</td>\n",
       "      <td>deep-purple-overture</td>\n",
       "      <td>2015</td>\n",
       "      <td>deep-purple</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Good golly, said little miss molly\\nWhen she w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67517</td>\n",
       "      <td>67517</td>\n",
       "      <td>paint-it-black</td>\n",
       "      <td>2015</td>\n",
       "      <td>deep-purple</td>\n",
       "      <td>Rock</td>\n",
       "      <td>I see a red door and I want it painted black,\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67523</td>\n",
       "      <td>67523</td>\n",
       "      <td>anyone-s-daughter</td>\n",
       "      <td>2014</td>\n",
       "      <td>deep-purple</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Well, I stood under your bedroom window, throw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67524</td>\n",
       "      <td>67524</td>\n",
       "      <td>perfect-strangers</td>\n",
       "      <td>2014</td>\n",
       "      <td>deep-purple</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Can you remember remember my name\\nAs I flow t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1551 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index                     song  year       artist genre  \\\n",
       "116793  116793  if-i-m-dreaming-my-life  2009  david-bowie  Rock   \n",
       "116794  116794                    seven  2009  david-bowie  Rock   \n",
       "116795  116795             i-can-t-read  2009  david-bowie  Rock   \n",
       "116796  116796         thursday-s-child  2009  david-bowie  Rock   \n",
       "116797  116797                  survive  2009  david-bowie  Rock   \n",
       "...        ...                      ...   ...          ...   ...   \n",
       "67509    67509            child-in-time  1972  deep-purple  Rock   \n",
       "67510    67510     deep-purple-overture  2015  deep-purple  Rock   \n",
       "67517    67517           paint-it-black  2015  deep-purple  Rock   \n",
       "67523    67523        anyone-s-daughter  2014  deep-purple  Rock   \n",
       "67524    67524        perfect-strangers  2014  deep-purple  Rock   \n",
       "\n",
       "                                                   lyrics  \n",
       "116793  VERSE (there)\\nWas she never there/here?\\nWas ...  \n",
       "116794  I forgot what my father said\\nI forgot what he...  \n",
       "116795  I can't read and I can't write down\\nI don't k...  \n",
       "116796  All of my life I've tried so hard\\nDoing my be...  \n",
       "116797  Oh, my\\nNaked eyes\\nI should have kept you\\nI ...  \n",
       "...                                                   ...  \n",
       "67509   Sweet child in time\\nYou'll see the line\\nThe ...  \n",
       "67510   Good golly, said little miss molly\\nWhen she w...  \n",
       "67517   I see a red door and I want it painted black,\\...  \n",
       "67523   Well, I stood under your bedroom window, throw...  \n",
       "67524   Can you remember remember my name\\nAs I flow t...  \n",
       "\n",
       "[1551 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artists = ['bruce-springsteen', 'elliot-smith', 'black-sabbath', 'deep-purple', 'david-bowie']\n",
    "\n",
    "david_bowie = songs[songs[u'artist'] == 'david-bowie']\n",
    "black_sabbath = songs[songs[u'artist'] == 'black-sabbath']\n",
    "bruce_springsteen = songs[songs[u'artist'] == 'bruce-springsteen']\n",
    "elliot_smith = songs[songs[u'artist'] == 'elliot-smith']\n",
    "deep_purple = songs[songs[u'artist'] == 'deep-purple']\n",
    "\n",
    "lyrics = pd.concat([david_bowie, black_sabbath, bruce_springsteen, elliot_smith, deep_purple], axis=0)\n",
    "lyrics = lyrics.dropna()\n",
    "lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['oh', 'yeah', 'hey', 'doo', 'oo', 'uh', 'la', 'verse', 'chorus', 'bridge', 'x2', \"'m\"]) # filter out common meaningless words/sounds and words describing song structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing pipeline we decided to implement includes the following steps:\n",
    "- filtering out stopwords, puntuation, sounds typical from songs\n",
    "- lemmatizing\n",
    "- adding the most common bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wa',\n",
       " 'never',\n",
       " 'there/here',\n",
       " 'wa',\n",
       " 'ever',\n",
       " 'wa',\n",
       " 'air',\n",
       " 'breathed',\n",
       " 'wrong',\n",
       " 'time',\n",
       " 'ohoh',\n",
       " 'ohoh',\n",
       " 'flower',\n",
       " 'gallery',\n",
       " 'hymn',\n",
       " 'night',\n",
       " 'singing',\n",
       " 'come',\n",
       " 'wrong',\n",
       " 'time',\n",
       " 'wrong',\n",
       " 'day',\n",
       " 'light',\n",
       " 'fading',\n",
       " 'dreaming',\n",
       " 'life',\n",
       " 'second',\n",
       " 'time',\n",
       " 'one',\n",
       " 'living',\n",
       " 'chance',\n",
       " 'mother',\n",
       " 'sigh',\n",
       " 'father',\n",
       " 'step',\n",
       " 'aside',\n",
       " 'wrong',\n",
       " 'time',\n",
       " 'ohoh',\n",
       " 'wa',\n",
       " 'ever',\n",
       " 'wa',\n",
       " 'ever',\n",
       " 'dreaming',\n",
       " 'life',\n",
       " 'dreaming',\n",
       " 'life',\n",
       " 'away',\n",
       " 'dreaming',\n",
       " 'life',\n",
       " 'dreaming',\n",
       " 'dreaming',\n",
       " 'dreaming',\n",
       " 'dreaming',\n",
       " 'life',\n",
       " 'dreaming',\n",
       " 'life',\n",
       " 'away',\n",
       " 'ohoh',\n",
       " 'repeat',\n",
       " 'dreaming',\n",
       " 'life',\n",
       " 'dreaming',\n",
       " 'dreaming',\n",
       " 'life',\n",
       " 'away',\n",
       " 'ohoh',\n",
       " 'are_fading',\n",
       " 'come_to',\n",
       " 'fading_now',\n",
       " 'father_step',\n",
       " 'flower_so',\n",
       " 'gallery_with',\n",
       " 'hymn_of',\n",
       " 'it_air',\n",
       " 'just_one',\n",
       " 'light_are',\n",
       " 'living_chance',\n",
       " 'mother_sigh',\n",
       " 'never_there/here',\n",
       " 'night_singing',\n",
       " 'of_night',\n",
       " 'one_living',\n",
       " 'singing_come',\n",
       " 'so_from',\n",
       " 'step_aside',\n",
       " 'to_me']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "\n",
    "def convertTuple(tup): \n",
    "    str =  '_'.join(tup) \n",
    "    return str\n",
    "  \n",
    "def simple_preprocess(lyrics, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    lyricslist = [re.sub('\\-', '', str(lyric)) for lyric in lyrics['lyrics'].tolist()] # take out all hyphens that often connect meaningless words/sounds to these stopwords can be filtered out later\n",
    "    lyricslist = [re.sub('[\\.\\,\\?,\\!,\\(,\\),\\:,\\\"]', '', str(lyric)) for lyric in lyricslist] # take out all punctuation\n",
    "    lyricslist = [word_tokenize(lyric.lower()) for lyric in lyricslist]\n",
    "    finder = []\n",
    "    for i,lyric in enumerate(lyricslist):\n",
    "        for j,token in enumerate(lyric):\n",
    "            lyricslist[i][j] = wnl.lemmatize(str(token))\n",
    "        lyricslist[i] = [word for word in lyric if word not in stop_words]\n",
    "        finder.append(BigramCollocationFinder.from_words(lyric))\n",
    "        finder[i] = finder[i].nbest(bigram_measures.pmi, 20)\n",
    "        finder[i] = [convertTuple(x) for x in finder[i]] #need to append the two words back together\n",
    "        lyricslist[i] = lyricslist[i] + finder[i]\n",
    "    lyrics['bag_of_words'] = lyricslist\n",
    "\n",
    "\n",
    "simple_preprocess(lyrics)\n",
    "example = lyrics['bag_of_words'][116793]\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 common topics between all songs by all artits\n",
    "This first part is an attempt to identify some common topics, in order to have a baseline of\n",
    "general musical topics that often appear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 31189\n"
     ]
    }
   ],
   "source": [
    "#count how many times a word appears --> give a document\n",
    "#lyrics['bag_of_words']\n",
    "\n",
    "list_BOWlyrics = [[]]\n",
    "for lyric in lyrics['bag_of_words']:\n",
    "    list_BOWlyrics += [lyric]\n",
    "\n",
    "lyrics_dictionary = corpora.Dictionary(list_BOWlyrics)\n",
    "print('Number of unique tokens:', len(lyrics_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'air': 0, 'are_fading': 1, 'aside': 2, 'away': 3, 'breathed': 4, 'chance': 5, 'come': 6, 'come_to': 7, 'day': 8, 'dreaming': 9, 'ever': 10, 'fading': 11}\n",
      "word with id 8: day\n",
      "frequency of token 8: 415\n"
     ]
    }
   ],
   "source": [
    "print(dict(itertools.islice(lyrics_dictionary.token2id.items(), 12)))\n",
    "print(\"word with id 8:\", lyrics_dictionary[8])\n",
    "print(\"frequency of token 8:\", lyrics_dictionary.dfs[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 3046\n"
     ]
    }
   ],
   "source": [
    "# Filter out words that occur in less than 5 documents, or more than 70% of the documents.\n",
    "lyrics_dictionary.filter_extremes(no_below=2, no_above=0.7)\n",
    "print('Number of unique tokens:', len(lyrics_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents\n",
    "lyrics_bow_corpus = [lyrics_dictionary.doc2bow(d) for d in list_BOWlyrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 3), (3, 1), (4, 1), (5, 1), (6, 12), (7, 3), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 8), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 4), (26, 5), (27, 4)]\n"
     ]
    }
   ],
   "source": [
    "# the BOW representation of the first document\n",
    "print(lyrics_bow_corpus[1][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dreaming --> 12\n",
      "life --> 8\n",
      "wa --> 5\n",
      "time --> 4\n",
      "wrong --> 4\n",
      "away --> 3\n",
      "ever --> 3\n",
      "air --> 1\n",
      "aside --> 1\n",
      "chance --> 1\n",
      "come --> 1\n",
      "day --> 1\n",
      "fading --> 1\n",
      "father --> 1\n",
      "flower --> 1\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# which words (and how often) appear in the first document?\n",
    "for i, freq in sorted(lyrics_bow_corpus[1], key=itemgetter(1), reverse=True)[:15]:\n",
    "    print(lyrics_dictionary[i], \"-->\", freq)\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_ldamodel = models.ldamodel.LdaModel(lyrics_bow_corpus, num_topics=20, id2word = lyrics_dictionary, passes= 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  [('long', 0.03684884),\n",
       "   ('home', 0.028831301),\n",
       "   ('back', 0.02327016),\n",
       "   ('wa', 0.02144347),\n",
       "   ('never', 0.019309334),\n",
       "   ('bring', 0.017147055),\n",
       "   (\"'ll\", 0.016577782),\n",
       "   (\"'ve\", 0.011354292),\n",
       "   ('go', 0.010860615),\n",
       "   ('fire', 0.010695467),\n",
       "   ('ah', 0.010335342),\n",
       "   ('eye', 0.010149982),\n",
       "   (\"'em\", 0.0100763),\n",
       "   ('town', 0.009408002),\n",
       "   (\"'d\", 0.009140688)]),\n",
       " (8,\n",
       "  [('take', 0.05732043),\n",
       "   (\"n't\", 0.03149467),\n",
       "   ('come', 0.027449243),\n",
       "   ('right', 0.021535905),\n",
       "   (\"'re\", 0.016061736),\n",
       "   ('hand', 0.015427335),\n",
       "   ('heart', 0.012536857),\n",
       "   ('shine', 0.012062132),\n",
       "   ('love', 0.0116058355),\n",
       "   ('gon', 0.011558871),\n",
       "   ('na', 0.010237374),\n",
       "   ('see', 0.010018136),\n",
       "   ('say', 0.009503205),\n",
       "   ('rise', 0.009429461),\n",
       "   ('know', 0.008621004)]),\n",
       " (19,\n",
       "  [('ba', 0.030092109),\n",
       "   ('love', 0.026689498),\n",
       "   ('people', 0.01832417),\n",
       "   (']', 0.018083861),\n",
       "   ('[', 0.018014856),\n",
       "   ('man', 0.01770162),\n",
       "   ('instrumental', 0.016124912),\n",
       "   ('lie', 0.014542432),\n",
       "   ('telling', 0.011816175),\n",
       "   (\"'ll\", 0.011131483),\n",
       "   ('say', 0.010823389),\n",
       "   ('`', 0.010762494),\n",
       "   ('one', 0.01035023),\n",
       "   ('anything', 0.01020557),\n",
       "   ('give', 0.010192957)]),\n",
       " (14,\n",
       "  [(\"n't\", 0.117415555),\n",
       "   ('got', 0.054223597),\n",
       "   ('ai', 0.024234653),\n",
       "   (\"'ve\", 0.023713602),\n",
       "   ('ca', 0.019670613),\n",
       "   ('want', 0.016437551),\n",
       "   ('man', 0.013628092),\n",
       "   ('get', 0.013307086),\n",
       "   ('know', 0.01249602),\n",
       "   ('like', 0.01000795),\n",
       "   (\"'re\", 0.009553815),\n",
       "   ('mary', 0.0075528007),\n",
       "   ('wo', 0.0073518776),\n",
       "   (\"'cause\", 0.0072510294),\n",
       "   ('look', 0.006751838)]),\n",
       " (13,\n",
       "  [(\"'\", 0.073021024),\n",
       "   ('wa', 0.019371096),\n",
       "   ('well', 0.018661033),\n",
       "   ('man', 0.012762892),\n",
       "   ('come', 0.012197077),\n",
       "   ('na', 0.010827209),\n",
       "   ('little', 0.010784241),\n",
       "   ('light', 0.010291003),\n",
       "   ('got', 0.010060211),\n",
       "   (\"n't\", 0.009840396),\n",
       "   ('gon', 0.009333028),\n",
       "   (\"'d\", 0.009221728),\n",
       "   ('night', 0.008620856),\n",
       "   ('tonight', 0.0083128335),\n",
       "   ('said', 0.008245027)]),\n",
       " (2,\n",
       "  [('need', 0.020387124),\n",
       "   (\"n't\", 0.020308835),\n",
       "   ('night', 0.019479329),\n",
       "   ('dancing', 0.016863),\n",
       "   ('heart', 0.013741385),\n",
       "   ('black', 0.013190212),\n",
       "   ('love', 0.012170181),\n",
       "   ('morning', 0.011570029),\n",
       "   ('know', 0.011288318),\n",
       "   ('turn', 0.011146525),\n",
       "   (\"'ll\", 0.010181171),\n",
       "   ('real', 0.009633592),\n",
       "   ('way', 0.009516748),\n",
       "   ('man', 0.008514212),\n",
       "   ('free', 0.008082643)]),\n",
       " (7,\n",
       "  [('like', 0.021857608),\n",
       "   ('wa', 0.020019472),\n",
       "   ('nothing', 0.015723476),\n",
       "   ('life', 0.01333161),\n",
       "   ('name', 0.009790171),\n",
       "   ('day', 0.009339765),\n",
       "   ('star', 0.009186505),\n",
       "   ('could', 0.008808439),\n",
       "   ('gon', 0.008074294),\n",
       "   ('world', 0.008003903),\n",
       "   ('hero', 0.007973436),\n",
       "   ('na', 0.007709146),\n",
       "   ('one', 0.007446445),\n",
       "   ('ya', 0.007374118),\n",
       "   ('heaven', 0.006862657)]),\n",
       " (3,\n",
       "  [('give', 0.0332835),\n",
       "   (\"'ll\", 0.03240868),\n",
       "   ('boy', 0.0232211),\n",
       "   ('hold', 0.017530074),\n",
       "   ('u', 0.0167788),\n",
       "   ('thing', 0.016592573),\n",
       "   ('got', 0.0154847745),\n",
       "   ('may', 0.014140403),\n",
       "   ('hope', 0.014024112),\n",
       "   ('love', 0.013330247),\n",
       "   ('tonight', 0.012633721),\n",
       "   ('back', 0.008637472),\n",
       "   ('`', 0.008469292),\n",
       "   ('one', 0.008371063),\n",
       "   (\"'ve\", 0.008318187)]),\n",
       " (10,\n",
       "  [(\"n't\", 0.032606065),\n",
       "   ('na', 0.029221362),\n",
       "   (\"'re\", 0.026772909),\n",
       "   ('heart', 0.020960692),\n",
       "   (\"'\", 0.018664896),\n",
       "   ('make', 0.013723335),\n",
       "   ('wan', 0.013707622),\n",
       "   ('want', 0.013664324),\n",
       "   ('gon', 0.0134264305),\n",
       "   ('feel', 0.010315868),\n",
       "   ('young', 0.009614953),\n",
       "   ('johnny', 0.009446523),\n",
       "   ('afraid', 0.008920657),\n",
       "   ('know', 0.00868886),\n",
       "   ('alone', 0.008443017)]),\n",
       " (16,\n",
       "  [('night', 0.036672123),\n",
       "   (\"'ll\", 0.018650284),\n",
       "   ('sky', 0.018337479),\n",
       "   ('whoa', 0.01810728),\n",
       "   ('eye', 0.016741581),\n",
       "   ('let', 0.0157279),\n",
       "   ('river', 0.012744781),\n",
       "   ('love', 0.010271978),\n",
       "   ('boy', 0.009892734),\n",
       "   ('well', 0.009822535),\n",
       "   ('come', 0.009748206),\n",
       "   ('blood', 0.00964169),\n",
       "   ('sun', 0.009608984),\n",
       "   ('ride', 0.009188899),\n",
       "   (\"'re\", 0.008580941)])]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the setting formatted=False allows you to get rid of the word*probability format when retrieveing topics\n",
    "lyrics_ldamodel.show_topics(formatted=False, num_words=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 common topics between all songs per artist\n",
    "Now we are interested in identifying what are the common topics of each artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 14039\n",
      "Number of unique tokens: 1690\n",
      "Number of unique tokens: 5423\n",
      "Number of unique tokens: 6415\n",
      "Number of unique tokens: 13681\n"
     ]
    }
   ],
   "source": [
    "#count how many times a word appears --> give a document\n",
    "#lyrics['bag_of_words']\n",
    "list_BOWlyrics_per_artist = [] \n",
    "lyrics_dictionary_per_artist = []\n",
    "\n",
    "for i,artist in enumerate(artists):\n",
    "    lyrics_of_artist = []\n",
    "    for lyric in lyrics[lyrics['artist'] == artist]['bag_of_words']:\n",
    "        lyrics_of_artist += [lyric]\n",
    "    list_BOWlyrics_per_artist.append(lyrics_of_artist)\n",
    "\n",
    "for i,artist in enumerate(artists):\n",
    "    lyrics_dictionary_per_artist.append(corpora.Dictionary(list_BOWlyrics_per_artist[i]))\n",
    "    print('Number of unique tokens:', len(lyrics_dictionary_per_artist[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_bow_corpus_per_artist = []\n",
    "\n",
    "for i,artist in enumerate(artists):\n",
    "    lyrics_bow_corpus_per_artist.append([lyrics_dictionary_per_artist[i].doc2bow(d) \n",
    "                                         for d in list_BOWlyrics_per_artist[i]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_ldamodel_per_artist = []\n",
    "\n",
    "for i,artist in enumerate(artists):\n",
    "    lyrics_ldamodel_per_artist.append(models.ldamodel.LdaModel(lyrics_bow_corpus_per_artist[i],\\\n",
    "                                                               num_topics=20, \\\n",
    "                                                               id2word = lyrics_dictionary_per_artist[i],\\\n",
    "                                                               passes= 25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bruce-springsteen [(15, [('train', 0.034895256), ('give', 0.020677026), ('got', 0.018006422), (\"n't\", 0.016367648), ('hope', 0.015394403), ('love', 0.014963676), ('ooh', 0.0139214), ('u', 0.013627458), ('fun', 0.012805798), ('may', 0.012710164), ('carry', 0.011292292), (\"'ll\", 0.010942565), ('come', 0.009879748), ('take', 0.009471915), ('get', 0.008475069)]), (7, [('ho', 0.019080045), ('wa', 0.012753229), ('santa', 0.012539801), ('coming', 0.011752582), ('man', 0.011267436), ('town', 0.0111444155), ('claus', 0.010804578), ('sha', 0.010797463), ('heart', 0.009613637), (\"n't\", 0.009346355), (\"'\", 0.008933277), (\"'s\", 0.0084844185), ('come', 0.007041963), ('like', 0.006693133), ('sold', 0.006687723)]), (19, [(\"'\", 0.045649007), ('bring', 0.018815724), ('home', 0.015046288), (\"'em\", 0.014603241), ('li', 0.012171351), ('thinkin', 0.012170738), ('day', 0.010068078), ('wa', 0.009608982), (\"'s\", 0.0093703475), ('girl', 0.007986004), ('baby', 0.00751635), ('sandy', 0.00734085), (\"n't\", 0.0060648886), ('life', 0.0056690876), ('night', 0.0055181314)]), (13, [(\"'s\", 0.021903206), ('baby', 0.016371801), ('cain', 0.01593139), ('raised', 0.015316512), ('adam', 0.014725469), ('come', 0.012702374), ('let', 0.010463975), (\"n't\", 0.010053428), (\"'re\", 0.0099676065), ('know', 0.009658711), (\"'\", 0.008140036), ('friend', 0.007979182), ('love', 0.007673247), ('got', 0.007197617), ('way', 0.0068298364)]), (6, [('need', 0.02288644), ('heart', 0.022885703), ('never', 0.01375413), ('confession', 0.011471582), ('depression', 0.011471354), (\"'ve\", 0.008048023), ('ive', 0.008046406), ('lost', 0.004622831), ('low', 0.0046228105), ('morning', 0.0046227267), ('sun', 0.0046226326), ('baby', 0.0046225935), ('always', 0.0023399957), ('nothing', 0.0023399014), ('shaken', 0.0023398723)]), (16, [('love', 0.023695724), (\"'s\", 0.021844793), (\"'\", 0.018393608), ('come', 0.017832998), ('away', 0.010858757), (\"n't\", 0.0102423085), ('baby', 0.009981326), ('got', 0.009830431), ('back', 0.009305175), ('dream', 0.008159444), ('well', 0.007579843), ('day', 0.0069591207), ('better', 0.0062761405), ('wa', 0.006145418), ('take', 0.0058170175)]), (2, [(\"'\", 0.039090335), (\"'s\", 0.027573388), (\"n't\", 0.014887632), ('well', 0.012465636), ('come', 0.012241683), ('got', 0.010591954), ('baby', 0.009819383), ('tonight', 0.008710082), (\"'ll\", 0.008387294), ('wa', 0.008317493), ('na', 0.0071253856), ('man', 0.0061127734), ('like', 0.005882303), ('let', 0.005877374), ('little', 0.0058691246)]), (14, [('shackled', 0.01922674), ('drawn', 0.017512815), ('keep', 0.012973627), (\"'s\", 0.012863819), ('hold', 0.012654652), ('eye', 0.012545793), (\"'ll\", 0.009328748), ('river', 0.007703473), ('wa', 0.007614822), ('prize', 0.0071642254), ('land', 0.006614851), ('american', 0.0066059376), ('man', 0.006277774), ('hand', 0.005822316), ('go', 0.005660281)]), (5, [('night', 0.024127033), ('light', 0.014758711), ('party', 0.01230784), ('miss', 0.012307734), (\"'\", 0.012043047), ('let', 0.008557623), (\"'s\", 0.0083637955), (\"'ve\", 0.00798838), ('wa', 0.007752329), (\"'re\", 0.007253311), ('linda', 0.006847903), ('one', 0.0064746924), ('well', 0.0058915536), ('swallowed', 0.0054832962), ('wage', 0.005482833)]), (0, [('na', 0.019032892), (\"'re\", 0.0143892765), (\"'\", 0.014165791), ('gon', 0.012526771), (\"'s\", 0.012404448), ('get', 0.010610999), ('need', 0.010084583), ('home', 0.0088238325), (\"n't\", 0.008687705), ('shooter', 0.008243967), ('long', 0.007343448), ('jeannie', 0.007275222), ('little', 0.0065766987), ('wa', 0.006432982), ('band', 0.0063532125)])] \n",
      "\n",
      "elliot-smith [(7, [('word', 0.011985572), ('speak', 0.011985175), ('talking', 0.011984659), ('killing', 0.00060245494), ('cry', 0.00059707527), ('time', 0.0005893099), ('wo', 0.0005887847), ('please_say', 0.0005882488), (\"n't\", 0.00058456295), ('stop', 0.00058427546), ('strip', 0.00058334786), ('ground', 0.00058210094), ('fine', 0.0005820323), ('name', 0.00058076), ('kicked', 0.0005800146)]), (12, [('morning', 0.02844696), ('around', 0.022815935), (\"n't\", 0.017182983), (\"'ll\", 0.011550963), ('want', 0.011548656), ('feel', 0.011548188), ('say', 0.011548134), (\"'s\", 0.011547895), ('know', 0.011547267), ('see', 0.011546851), ('tell', 0.0059158974), ('like', 0.00591553), ('ca', 0.0059154183), ('best', 0.005915333), ('one', 0.00591527)]), (1, [(\"'s\", 0.06354428), ('gon', 0.025356501), ('na', 0.025343822), ('lit', 0.023550734), (\"'ve\", 0.0217473), ('lie', 0.016333582), ('around', 0.016205523), ('know', 0.014583824), ('never', 0.013523032), ('son', 0.0127242), ('love', 0.010919429), (\"'re\", 0.010433365), (\"n't\", 0.0098407995), ('anyhow', 0.009114369), ('make', 0.0076273936)]), (19, [('know', 0.037163503), (\"n't\", 0.018390065), (\"'s\", 0.014682764), ('home', 0.012689998), ('never', 0.010633123), ('oblivion', 0.009892553), ('make', 0.009648514), ('come', 0.008487729), ('took', 0.00848317), (\"'re\", 0.008193617), ('away', 0.007274816), (\"'cause\", 0.007088067), ('well', 0.0070872777), ('ok', 0.0070869895), ('day', 0.007077402)]), (10, [('wrong', 0.03376521), ('see', 0.02704121), (\"'s\", 0.023692582), ('nothing', 0.02032178), ('dying', 0.017023344), ('everybody', 0.013615875), ('get', 0.0136044), ('disease', 0.01360373), (\"'ll\", 0.010264962), ('tired', 0.010250892), ('never', 0.010245702), ('sick', 0.010245197), ('really', 0.010244731), ('picture', 0.010244705), ('surprised', 0.010244648)]), (14, [('whisper', 0.00059209624), (\"'s\", 0.00059203064), ('driving', 0.00059200864), ('oldsmobile', 0.0005920018), ('past', 0.0005919912), ('condor', 0.00059196557), ('avenue', 0.00059193285), ('like', 0.00059192494), ('never', 0.00059192383), ('get', 0.00059191557), ('took', 0.00059190666), ('car', 0.0005919053), ('make', 0.0005919), (\"'ll\", 0.0005918947), ('smoke', 0.00059186533)]), (16, [('amity', 0.10058594), ('go', 0.041591175), ('good', 0.012093923), ('hello', 0.012093784), (\"'cos\", 0.01209377), ('make', 0.012093667), (\"'s\", 0.012093121), ('talk', 0.006194348), ('god', 0.0061943433), ('neon_sign', 0.0061943405), ('plain', 0.0061943186), ('junk_but', 0.0061943135), ('world', 0.0061943093), ('kitty_happy', 0.006194306), ('but_it', 0.006194306)]), (5, [(\"n't\", 0.018417045), (\"'s\", 0.015373717), (\"'d\", 0.015372322), ('apart', 0.012328348), ('someone', 0.012328338), ('work', 0.012328265), ('art', 0.012328244), ('half', 0.0123282075), ('put', 0.012328177), ('smart', 0.012328094), ('ca', 0.009284555), ('start', 0.009284495), ('wa', 0.009284417), ('say', 0.009284397), ('adeline', 0.009284385)]), (9, [('baby', 0.011395559), ('center', 0.011395556), ('circle', 0.0113954805), ('back', 0.011395032), ('walking', 0.011394458), ('away', 0.011322517), ('looking', 0.011120586), ('going', 0.011084579), (\"'ll\", 0.01106283), (\"n't\", 0.008737065), ('black', 0.0058381543), ('fade', 0.005837677), ('waking', 0.0058368137), ('harm', 0.0058367993), ('everybody_wa', 0.0058367867)]), (18, [(\"n't\", 0.03285539), ('alone', 0.02450956), (\"'s\", 0.023722144), ('know', 0.020357264), ('leave', 0.020356655), ('belong', 0.020356532), ('day', 0.01699244), ('go', 0.01362903), ('could', 0.013626182), ('looking', 0.013625827), ('everybody', 0.0102629755), ('saw', 0.010262511), ('independence', 0.010262335), ('tomorrow', 0.010262212), (\"'ll\", 0.010259494)])] \n",
      "\n",
      "black-sabbath [(17, [(\"'s\", 0.024535004), ('king', 0.014346946), ('say', 0.010277085), ('hold', 0.008260542), ('fool', 0.008251741), (\"'re\", 0.0082456935), ('blood', 0.008241786), ('mob', 0.008241448), ('rule', 0.008241324), ('light', 0.006212672), (\"'ve\", 0.0062083136), ('end', 0.006207989), ('wing', 0.006206882), ('away', 0.0062067783), ('eternity', 0.0062066754)]), (8, [(\"'s\", 0.022775307), (\"n't\", 0.0122817), ('love', 0.011144739), ('dead', 0.011034278), ('god', 0.0109612355), ('never', 0.0077074883), ('neon', 0.0076808296), ('time', 0.0067899553), ('cry', 0.006724861), ('take', 0.006104763), ('mind', 0.0060785515), ('come', 0.00604113), ('go', 0.006040905), ('light', 0.006040789), ('know', 0.0053121164)]), (15, [(\"'s\", 0.034085628), (\"n't\", 0.020199256), ('say', 0.010788797), ('see', 0.010397569), ('know', 0.009990152), (\"'ll\", 0.009393264), ('think', 0.0078016124), ('life', 0.0075845593), ('way', 0.0075753), ('still', 0.0072934423), ('day', 0.007104965), (\"'re\", 0.00641841), ('one', 0.0063157515), ('night', 0.0058306404), ('mind', 0.005165705)]), (6, [(\"'s\", 0.01994935), ('life', 0.013591611), ('love', 0.01222149), ('time', 0.0099489475), ('one', 0.0091165835), ('another', 0.0084373895), ('hard', 0.007492211), ('road', 0.006574028), ('leave', 0.006310506), (\"n't\", 0.006303682), (\"'re\", 0.0057901107), ('look', 0.0055559413), ('world', 0.00547055), ('make', 0.0052678566), ('day', 0.0050961017)]), (16, [(\"'s\", 0.041565612), ('insane', 0.030838113), ('tell', 0.017637812), ('going', 0.017572856), ('people', 0.017106267), ('alright', 0.016738134), ('use', 0.00972334), (\"'ll\", 0.009341056), ('never', 0.007965953), ('time', 0.0078034527), (\"'ve\", 0.007643536), (\"n't\", 0.007507255), ('see', 0.007412677), ('place', 0.007083798), ('yes', 0.0053300844)]), (1, [(\"'s\", 0.020834448), (\"n't\", 0.009825163), ('world', 0.008901477), ('cross', 0.008492805), ('power', 0.0084512625), ('headless', 0.007974844), ('take', 0.007897755), ('na', 0.0077735875), ('away', 0.007144145), ('child', 0.0067532286), ('know', 0.006752967), ('back', 0.006732384), (\"'\", 0.006690096), ('illusion', 0.0066181505), ('get', 0.006587983)]), (0, [('see', 0.028905347), (\"'s\", 0.027360154), ('got', 0.024798019), ('ta', 0.019899573), ('doctor', 0.018244924), ('rock', 0.016589822), (\"'\", 0.01571995), (\"n'roll\", 0.014934912), ('away', 0.011222596), ('dream', 0.009952798), ('na', 0.009541834), ('get', 0.009357253), (\"'re\", 0.007846142), ('gon', 0.007841579), ('time', 0.006356195)]), (11, [(\"n't\", 0.047578778), (\"'s\", 0.0190125), ('na', 0.017906906), ('gon', 0.016409814), ('ai', 0.012875582), ('know', 0.011122378), ('life', 0.010292169), (\"'re\", 0.010114681), ('turn', 0.008775897), ('cry', 0.007697348), ('death', 0.007686356), ('want', 0.007641209), ('die', 0.00745654), ('soul', 0.0071769645), ('see', 0.007139864)]), (10, [('instrumental', 0.020087821), (\"'s\", 0.013294413), ('keep', 0.011782862), ('losing', 0.0067526614), ('never', 0.0066239615), ('people', 0.0060317293), ('take', 0.0056722546), ('edge', 0.0053011645), ('virtual', 0.005085537), ('death', 0.0050854664), ('walking', 0.0050854236), ('wizard', 0.0050854087), ('find', 0.0050854054), ('talking', 0.0050854003), ('state', 0.005085389)]), (3, [('walk', 0.041505583), ('nothing', 0.00867141), ('head', 0.007996492), ('away', 0.0076915147), ('turn', 0.0072351545), ('higher', 0.0061150407), ('right', 0.005410607), ('fire', 0.0036045574), ('sky_burn', 0.003131938), ('could_be', 0.003131863), ('look_of', 0.0031317957), ('burn_before', 0.0031317908), ('been_lonely', 0.0031317852), ('pleased', 0.003131784), ('sunlight', 0.0031317417)])] \n",
      "\n",
      "deep-purple [(3, [('getting', 0.0078116003), ('door', 0.0060117715), (\"'d\", 0.0054931603), ('heart', 0.0049829367), ('back', 0.0045425505), ('solitaire', 0.0041637826), ('despair', 0.0041637663), ('follow', 0.0041637104), ('stare', 0.0041637085), ('voice', 0.004163666), ('lived', 0.004163642), ('icy', 0.004163609), ('stranger', 0.0041635334), ('empty', 0.004163213), ('falling', 0.004162787)]), (10, [('aaaahh', 0.121150866), ('ooo', 0.060608257), ('aahh', 0.060607735), ('ooooooo', 0.040426373), ('oooooo', 0.02024455), ('see', 0.010153891), ('flying', 0.0101538235), ('bad', 0.010153726), (\"'ve\", 0.010153666), ('aah', 0.01015365), ('line', 0.010153588), (\"'s\", 0.0051091234), ('head', 0.0051087593), ('time', 0.0051087043), ('good', 0.00510853)]), (0, [('love', 0.02230079), ('back', 0.022300405), ('going', 0.012289958), ('said', 0.011178396), ('pain', 0.011177923), ('ever', 0.011177704), ('ice', 0.0067310752), ('need', 0.006729126), ('time', 0.0067290803), ('mitzi', 0.006728962), ('know', 0.0067288065), ('house', 0.006728804), ('see', 0.0056174933), ('fire', 0.0056170276), ('wa', 0.005616998)]), (7, [(\"'s\", 0.027420992), (\"n't\", 0.012279599), ('know', 0.0076979455), ('black', 0.0074081677), (\"'ll\", 0.0073394296), ('sky', 0.0071040825), ('white', 0.0067376005), ('like', 0.0064231884), (\"'\", 0.0057255775), ('well', 0.005681106), ('going', 0.0053969407), ('girl', 0.0053968835), ('let', 0.005093475), ('tell', 0.0050421352), ('got', 0.004730595)]), (19, [(\"'s\", 0.00015770763), ('said', 0.00015769947), ('wa', 0.00015724722), ('woman', 0.00015707605), (\"'\", 0.0001568799), (\"'re\", 0.0001567755), ('better', 0.00015669061), (\"n't\", 0.00015668212), ('gon', 0.00015663815), ('mitzi', 0.00015658265), ('going', 0.00015657324), (\"'d\", 0.000156545), ('time', 0.00015653674), ('wall', 0.00015653286), ('one', 0.00015651873)]), (18, [(\"'s\", 0.025034264), ('na', 0.021555873), ('gon', 0.020978963), (\"n't\", 0.01920061), (\"'re\", 0.013811832), ('wa', 0.012498206), ('got', 0.012145012), ('come', 0.011058825), (\"'\", 0.009934439), (\"'ve\", 0.00989671), ('ooh', 0.009733025), ('love', 0.009247089), ('want', 0.009130653), ('woman', 0.009027015), ('baby', 0.008139959)]), (12, [('time', 0.028390538), (\"'s\", 0.026108513), ('work', 0.016707525), ('way', 0.015763197), (\"n't\", 0.013146139), ('see', 0.013118855), ('wrong', 0.01289314), ('love', 0.0104348), ('man', 0.010100661), (\"'re\", 0.009089714), ('got', 0.008936408), ('right', 0.007489323), ('try', 0.006525587), ('think', 0.006484325), ('friend', 0.0061022355)]), (4, [(\"n't\", 0.03056354), (\"'s\", 0.027706575), ('got', 0.01627969), ('time', 0.011926432), (\"'ll\", 0.011050299), (\"'re\", 0.010219533), ('know', 0.00968971), ('around', 0.00889349), ('never', 0.008112785), (\"'ve\", 0.007666625), ('ai', 0.006697519), ('like', 0.006675926), ('see', 0.0065125767), ('go', 0.005945353), ('could', 0.0059036547)]), (15, [('alone', 0.021820603), ('anya', 0.018385544), ('away', 0.013803574), ('far', 0.011512521), ('ba', 0.010367103), ('day', 0.009114255), ('coming', 0.008478043), ('stormbringer', 0.008074966), ('like', 0.0075027025), ('heart', 0.006930424), ('gypsy', 0.0069303177), ('breaking', 0.006930256), ('keep', 0.006822973), (\"'s\", 0.0061444887), ('across', 0.0057847844)]), (8, [(\"'s\", 0.052038983), (\"n't\", 0.017584682), ('get', 0.010525847), ('man', 0.010458665), ('one', 0.009377544), ('got', 0.008489037), ('love', 0.0081421165), ('know', 0.0075508747), ('good', 0.007382158), ('wa', 0.007345212), ('want', 0.006967757), ('said', 0.0067064487), ('time', 0.006273589), ('never', 0.0061820117), ('head', 0.0061480836)])] \n",
      "\n",
      "david-bowie [(8, [(\"'s\", 0.034116834), (\"'re\", 0.019327955), (\"n't\", 0.013985607), (\"'ll\", 0.013725532), ('got', 0.011266454), ('boy', 0.010717108), ('thing', 0.009211961), ('get', 0.008360585), ('know', 0.007958679), ('somebody', 0.0076248283), ('like', 0.007539913), ('love', 0.007204213), ('make', 0.0061739627), ('let', 0.005936898), ('ah', 0.005816578)]), (13, [(\"'s\", 0.014117478), ('man', 0.013558943), ('looking', 0.013208391), ('wa', 0.012687872), ('like', 0.011411915), ('gone', 0.01084674), ('life', 0.010147191), ('set', 0.009863768), ('1984', 0.0086286375), ('world', 0.008552718), (\"n't\", 0.006937146), ('love', 0.006840611), ('could', 0.006644162), ('lust', 0.0064195623), ('would', 0.0062350566)]), (4, [('buh', 0.046504766), ('duh', 0.036174316), (\"'s\", 0.015734052), ('chingaling', 0.0135285575), ('na', 0.012733982), ('heart', 0.011541043), ('filthy', 0.011143816), ('lesson', 0.01114378), ('thing', 0.009901586), ('little', 0.0087756915), ('pretty', 0.008760749), ('footstompin', 0.0075685205), ('rock', 0.0074319895), ('game', 0.0067684148), ('something', 0.0067608557)]), (12, [('come', 0.030023009), (\"'re\", 0.01181492), (\"n't\", 0.011610966), (\"'s\", 0.010720903), (\"'d\", 0.010375597), ('rather', 0.009649117), ('back', 0.009068797), ('well', 0.008852326), ('got', 0.007673718), ('want', 0.006711952), (\"'ve\", 0.0067068944), ('make', 0.006505232), (\"'\", 0.006433873), ('turn', 0.00629864), ('home', 0.006212974)]), (14, [(\"'s\", 0.022865588), ('uhuhuh', 0.015888082), ('johnny', 0.014299025), ('got', 0.013937277), ('year', 0.013515892), ('afraid', 0.011126137), ('five', 0.011125927), (\"'ve\", 0.009226441), ('american', 0.008744192), ('uhuh', 0.007950595), (\"n't\", 0.0072075506), ('feel', 0.0067769075), ('want', 0.006085365), ('wa', 0.005572178), ('america', 0.0055693486)]), (0, [(\"n't\", 0.045077667), (\"'s\", 0.023882542), ('girl', 0.017052142), ('tell', 0.014087795), ('dancing', 0.012465153), (\"'ve\", 0.011961543), ('doe', 0.009954306), ('must', 0.009895362), ('know', 0.00914168), ('matter', 0.009052614), ('time', 0.0078067514), ('go', 0.0075134267), ('good', 0.007170262), ('got', 0.0068251933), ('love', 0.006789756)]), (9, [('[', 0.021406172), (']', 0.020179149), (\"n't\", 0.018511001), ('got', 0.018268496), (\"'s\", 0.015455385), (\"'ll\", 0.01531796), ('say', 0.015163128), ('better', 0.012651536), (\"'ve\", 0.011827436), ('go', 0.011469598), ('love', 0.010696931), ('shine', 0.010555337), ('girl', 0.010200975), ('way', 0.010070319), ('want', 0.008716727)]), (7, [(\"n't\", 0.021182964), ('love', 0.015157132), (\"'s\", 0.012857394), ('girl', 0.010294009), ('little', 0.009151086), ('god', 0.00879764), ('wa', 0.008350937), (\"'re\", 0.008165818), ('people', 0.0064293304), ('world', 0.0058911354), ('like', 0.0057215914), ('give', 0.005719286), ('never', 0.0057180733), ('falling', 0.00568211), ('ha', 0.005512097)]), (19, [(\"n't\", 0.014488066), (\"'s\", 0.013730567), ('nothing', 0.013470663), ('one', 0.0111754965), ('wa', 0.009706854), ('could', 0.009030718), ('hero', 0.008238114), ('day', 0.00813439), ('ca', 0.007757401), ('like', 0.0077291355), ('fall', 0.007157615), ('real', 0.006175649), ('get', 0.006135679), ('u', 0.0059804735), ('cruise', 0.005888847)]), (6, [(\"'s\", 0.021226365), (\"n't\", 0.012626226), ('like', 0.011447491), ('day', 0.010428777), ('come', 0.009914389), ('wa', 0.009472832), ('time', 0.008828188), ('night', 0.008491374), ('make', 0.008483569), ('ha', 0.008088403), ('love', 0.00796625), ('ba', 0.0070111593), (\"'re\", 0.0064973845), ('eye', 0.0063858987), ('heart', 0.0062843086)])] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the setting formatted=False allows you to get rid of the word*probability format when retrieveing topics\n",
    "for i,artist in enumerate(artists):\n",
    "    print(artist, lyrics_ldamodel_per_artist[i].show_topics(formatted=False, num_words=15), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
