{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lucovica Schaerf, Antònio Mendes, Jaël Kortekaas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large part of our code is used from: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the the preprocessing pipeline to our project. \n",
    "As a first step we are importing the data and filtering out all the songs that\n",
    "we don't need for our analysis. Secondly, we will implement the 'standard' \n",
    "pipeline and, once we obtain the most common words per each album, author, year\n",
    "(...) we will move to another file to do the clustering and topic analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.collocations import *\n",
    "from nltk.collocations import BigramAssocMeasures\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import gensim\n",
    "import spacy\n",
    "import itertools\n",
    "\n",
    "from gensim import corpora, models\n",
    "from nltk.corpus import wordnet as wn\n",
    "from operator import itemgetter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_dir = Path(\"./figures\")\n",
    "data_dir = Path(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/lyrics.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-12a2b1441d9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msongs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/lyrics.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0msongs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/lyrics.csv'"
     ]
    }
   ],
   "source": [
    "songs = []\n",
    "\n",
    "with open('./data/lyrics.csv', 'r', encoding=\"utf-8\") as infile:\n",
    "    songs = pd.read_csv(infile)\n",
    "    \n",
    "print(songs.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = ['bruce-springsteen', 'elliot-smith', 'black-sabbath', 'deep-purple', 'david-bowie']\n",
    "\n",
    "david_bowie = songs[songs[u'artist'] == 'david-bowie']\n",
    "black_sabbath = songs[songs[u'artist'] == 'black-sabbath']\n",
    "bruce_springsteen = songs[songs[u'artist'] == 'bruce-springsteen']\n",
    "elliot_smith = songs[songs[u'artist'] == 'elliot-smith']\n",
    "deep_purple = songs[songs[u'artist'] == 'deep-purple']\n",
    "\n",
    "lyrics = pd.concat([david_bowie, black_sabbath, bruce_springsteen, elliot_smith, deep_purple], axis=0)\n",
    "lyrics = lyrics.dropna()\n",
    "lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['oh', 'yeah', 'hey', 'doo', 'oo', 'uh', 'la', 'verse', 'chorus', 'bridge', 'x2', \"'m\", 'da', 'ooh', 'aaaahh', 'ooo', 'duh', 'whop', 'u', 'ah', 'na', 'whoa', 'ai', \"n't\", 'wa', 'gon', \"'ll\", 'gon', \"'d\", \"'re\", \"'ve\", \"'em\", \"'\", 'ca', 'ha', 'wo', 'wir', 'wan', 'doe', 'well', 'sha', 'ya', 'ta', \"'cause\", \"`\"]) # filter out common meaningless words/sounds and words describing song structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing pipeline we decided to implement includes the following steps:\n",
    "- filtering out stopwords, puntuation, sounds typical from songs\n",
    "- lemmatizing\n",
    "- adding the most common bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['never',\n",
       " 'there/here',\n",
       " 'ever',\n",
       " 'air',\n",
       " 'breathed',\n",
       " 'wrong',\n",
       " 'time',\n",
       " 'ohoh',\n",
       " 'ohoh',\n",
       " 'flower',\n",
       " 'gallery',\n",
       " 'hymn',\n",
       " 'night',\n",
       " 'singing',\n",
       " 'come',\n",
       " 'wrong',\n",
       " 'time',\n",
       " 'wrong',\n",
       " 'day',\n",
       " 'light',\n",
       " 'fading',\n",
       " 'dreaming',\n",
       " 'life',\n",
       " 'second',\n",
       " 'time',\n",
       " 'one',\n",
       " 'living',\n",
       " 'chance',\n",
       " 'mother',\n",
       " 'sigh',\n",
       " 'father',\n",
       " 'step',\n",
       " 'aside',\n",
       " 'wrong',\n",
       " 'time',\n",
       " 'ohoh',\n",
       " 'ever',\n",
       " 'ever',\n",
       " 'dreaming',\n",
       " 'life',\n",
       " 'dreaming',\n",
       " 'life',\n",
       " 'away',\n",
       " 'dreaming',\n",
       " 'life',\n",
       " 'dreaming',\n",
       " 'dreaming',\n",
       " 'dreaming',\n",
       " 'dreaming',\n",
       " 'life',\n",
       " 'dreaming',\n",
       " 'life',\n",
       " 'away',\n",
       " 'ohoh',\n",
       " 'repeat',\n",
       " 'dreaming',\n",
       " 'life',\n",
       " 'dreaming',\n",
       " 'dreaming',\n",
       " 'life',\n",
       " 'away',\n",
       " 'ohoh',\n",
       " 'are_fading',\n",
       " 'come_to',\n",
       " 'fading_now',\n",
       " 'father_step',\n",
       " 'flower_so',\n",
       " 'gallery_with',\n",
       " 'hymn_of',\n",
       " 'it_air',\n",
       " 'just_one',\n",
       " 'light_are',\n",
       " 'living_chance',\n",
       " 'mother_sigh',\n",
       " 'never_there/here',\n",
       " 'night_singing',\n",
       " 'of_night',\n",
       " 'one_living',\n",
       " 'singing_come',\n",
       " 'so_from',\n",
       " 'step_aside',\n",
       " 'to_me']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "\n",
    "def convertTuple(tup): \n",
    "    str =  '_'.join(tup) \n",
    "    return str\n",
    "  \n",
    "def simple_preprocess(lyrics, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    lyricslist = [re.sub('\\-', '', str(lyric)) for lyric in lyrics['lyrics'].tolist()] # take out all hyphens that often connect meaningless words/sounds to these stopwords can be filtered out later\n",
    "    lyricslist = [re.sub('[\\.\\,\\?,\\!,\\(,\\),\\:,\\\",\\[,\\]]', '', str(lyric)) for lyric in lyricslist] # take out all punctuation\n",
    "    lyricslist = [word_tokenize(lyric.lower()) for lyric in lyricslist]\n",
    "    finder = []\n",
    "    for i,lyric in enumerate(lyricslist):\n",
    "        for j,token in enumerate(lyric):\n",
    "            lyricslist[i][j] = wnl.lemmatize(str(token))\n",
    "        lyricslist[i] = [word for word in lyric if word not in stop_words]\n",
    "        finder.append(BigramCollocationFinder.from_words(lyric))\n",
    "        finder[i] = finder[i].nbest(bigram_measures.pmi, 20)\n",
    "        finder[i] = [convertTuple(x) for x in finder[i]] #need to append the two words back together\n",
    "        lyricslist[i] = lyricslist[i] + finder[i]\n",
    "    lyrics['bag_of_words'] = lyricslist\n",
    "\n",
    "\n",
    "simple_preprocess(lyrics)\n",
    "example = lyrics['bag_of_words'][116793]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 31161\n"
     ]
    }
   ],
   "source": [
    "#count how many times a word appears --> give a document\n",
    "#lyrics['bag_of_words']\n",
    "\n",
    "list_BOWlyrics = [[]]\n",
    "for lyric in lyrics['bag_of_words']:\n",
    "    list_BOWlyrics += [lyric]\n",
    "\n",
    "lyrics_dictionary = corpora.Dictionary(list_BOWlyrics)\n",
    "print('Number of unique tokens:', len(lyrics_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'air': 0, 'are_fading': 1, 'aside': 2, 'away': 3, 'breathed': 4, 'chance': 5, 'come': 6, 'come_to': 7, 'day': 8, 'dreaming': 9, 'ever': 10, 'fading': 11}\n",
      "word with id 8: day\n",
      "frequency of token 8: 415\n"
     ]
    }
   ],
   "source": [
    "print(dict(itertools.islice(lyrics_dictionary.token2id.items(), 12)))\n",
    "print(\"word with id 8:\", lyrics_dictionary[8])\n",
    "print(\"frequency of token 8:\", lyrics_dictionary.dfs[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 10555\n"
     ]
    }
   ],
   "source": [
    "# Filter out words that occur in less than 5 documents, or more than 70% of the documents.\n",
    "lyrics_dictionary.filter_extremes(no_below=2, no_above=0.3)\n",
    "print('Number of unique tokens:', len(lyrics_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents\n",
    "lyrics_bow_corpus = [lyrics_dictionary.doc2bow(d) for d in list_BOWlyrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 3), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 12), (9, 3), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 8), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 4)]\n"
     ]
    }
   ],
   "source": [
    "# the BOW representation of the first document\n",
    "print(lyrics_bow_corpus[1][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dreaming --> 12\n",
      "life --> 8\n",
      "wrong --> 4\n",
      "away --> 3\n",
      "ever --> 3\n",
      "air --> 1\n",
      "aside --> 1\n",
      "breathed --> 1\n",
      "chance --> 1\n",
      "come --> 1\n",
      "come_to --> 1\n",
      "day --> 1\n",
      "fading --> 1\n",
      "father --> 1\n",
      "flower --> 1\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# which words (and how often) appear in the first document?\n",
    "for i, freq in sorted(lyrics_bow_corpus[1], key=itemgetter(1), reverse=True)[:15]:\n",
    "    print(lyrics_dictionary[i], \"-->\", freq)\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_ldamodel = models.ldamodel.LdaModel(lyrics_bow_corpus, num_topics=5, id2word = lyrics_dictionary, passes=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [('come', 0.022228166),\n",
       "   ('dream', 0.010456898),\n",
       "   ('fame', 0.008968287),\n",
       "   ('sex', 0.0072653196),\n",
       "   ('train', 0.007106955),\n",
       "   ('thing', 0.0068858913),\n",
       "   ('baby', 0.0066452166),\n",
       "   ('see', 0.006549559),\n",
       "   ('heart', 0.0059301187),\n",
       "   ('man', 0.005518989),\n",
       "   ('mary', 0.005500325),\n",
       "   ('get', 0.005296715),\n",
       "   ('five', 0.005001182),\n",
       "   ('weep', 0.0049423715),\n",
       "   ('church', 0.0045934506)]),\n",
       " (1,\n",
       "  [('life', 0.009311324),\n",
       "   ('never', 0.0085566165),\n",
       "   ('take', 0.008237282),\n",
       "   ('see', 0.0074760015),\n",
       "   ('say', 0.0073133465),\n",
       "   ('want', 0.0073046787),\n",
       "   ('nothing', 0.006593459),\n",
       "   ('feel', 0.006163559),\n",
       "   ('eye', 0.0060771205),\n",
       "   ('tell', 0.0059051607),\n",
       "   ('heart', 0.005699982),\n",
       "   ('go', 0.005409036),\n",
       "   ('man', 0.0053820666),\n",
       "   ('world', 0.00535927),\n",
       "   ('look', 0.005178884)]),\n",
       " (2,\n",
       "  [('let', 0.011314734),\n",
       "   ('day', 0.010959643),\n",
       "   ('get', 0.007982722),\n",
       "   ('way', 0.007578744),\n",
       "   ('alright', 0.00611332),\n",
       "   ('girl', 0.0060726483),\n",
       "   ('better', 0.005926346),\n",
       "   ('good', 0.005884422),\n",
       "   ('boy', 0.005208018),\n",
       "   ('could', 0.005163739),\n",
       "   ('say', 0.0046898797),\n",
       "   ('said', 0.004477603),\n",
       "   ('heart', 0.0044364478),\n",
       "   ('thing', 0.004198886),\n",
       "   ('eye', 0.0041801427)]),\n",
       " (3,\n",
       "  [('dance', 0.009362019),\n",
       "   ('baby', 0.009221613),\n",
       "   ('take', 0.009103236),\n",
       "   ('girl', 0.008833499),\n",
       "   ('long', 0.008239275),\n",
       "   ('little', 0.008218332),\n",
       "   ('come', 0.007509898),\n",
       "   ('want', 0.0074842405),\n",
       "   ('magic', 0.00626719),\n",
       "   ('get', 0.0060766283),\n",
       "   ('shot', 0.005874428),\n",
       "   ('man', 0.0055669183),\n",
       "   ('see', 0.005348698),\n",
       "   ('way', 0.0051378165),\n",
       "   ('turn', 0.0049385857)]),\n",
       " (4,\n",
       "  [('baby', 0.013941798),\n",
       "   ('come', 0.0107276),\n",
       "   ('back', 0.010218502),\n",
       "   ('go', 0.008085981),\n",
       "   ('little', 0.0077755265),\n",
       "   ('man', 0.007149453),\n",
       "   ('home', 0.0071085645),\n",
       "   ('hand', 0.006694924),\n",
       "   ('away', 0.005915064),\n",
       "   ('get', 0.005801412),\n",
       "   ('never', 0.005490629),\n",
       "   ('girl', 0.0053781318),\n",
       "   ('tonight', 0.0050975247),\n",
       "   ('make', 0.004897425),\n",
       "   ('right', 0.0046632974)])]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the setting formatted=False allows you to get rid of the word*probability format when retrieveing topics\n",
    "lyrics_ldamodel.show_topics(formatted=False, num_words=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import nltk.corpus\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "from gensim import corpora, models\n",
    "from nltk.corpus import wordnet as wn\n",
    "from operator import itemgetter\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics, manifold\n",
    "import scipy\n",
    "from scipy import cluster\n",
    "import matplotlib as mpl \n",
    "import matplotlib.cm as cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
